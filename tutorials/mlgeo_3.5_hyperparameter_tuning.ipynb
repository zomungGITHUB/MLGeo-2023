{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Hyperparameter Tuning\n",
    "\n",
    "The choice of model parameters that is optimal for the specific problem can be found automatically by searching the model parameter space. The type of algorithm is fixed in this instance.\n",
    "\n",
    "There are built-in toolkits to perform the hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wget\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use the classification example of event discrimination using seismic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget.download(\"https://www.dropbox.com/s/qwo2rh5zqwho1l5/miniPNW_metadata.csv?dl=1\")\n",
    "wget.download(\"https://www.dropbox.com/s/ie34kfu33d2jv9m/miniPNW_waveforms.hdf5?dl=1\")\n",
    "os.replace(\"miniPNW_metadata.csv\",\"../../miniPNW_metadata.csv\")\n",
    "os.replace(\"miniPNW_waveforms.hdf5\",\"../../miniPNW_waveforms.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the metadata. We will pick the source_type as a categorical class that we aim to classify the data into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read the metadata\n",
    "df = pd.read_csv(\"../../miniPNW_metadata.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the array of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['source_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many classes are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(labels))\n",
    "print(len(np.unique(labels)))\n",
    "plt.hist(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 classes. The data looks quite imbalanced. This is because the seismic networks will preferably pick earthquakes.\n",
    "\n",
    "Now are read the data. It is stored in an HDF5 files under a finite number of groups. Each groups has an array of datasets that correspond to the waveforms. To link the metadata to the waveform files, the key ``trace_name`` has the dataset ID. The address is labeled as follows: \n",
    "\n",
    "bucketX$i,:3,:n\n",
    "\n",
    "where X is the HDF5 group number, i is the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"../../miniPNW_waveforms.hdf5\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(tn,f):\n",
    "    bucket, narray = tn.split('$')\n",
    "    x, y, z = iter([int(i) for i in narray.split(',:')])\n",
    "    print(f['/data/%s' % bucket].shape)\n",
    "    data = f['/data/%s' % bucket][x, :y, :z]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldata=list(df['trace_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crap=read_data(ldata[40],f)\n",
    "print(crap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(crap[2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just extract the Z component and reshape them into a single array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt=crap.shape[-1]\n",
    "ndata=len(labels)\n",
    "print(ndata,nt)\n",
    "Z=np.zeros(shape=(ndata,nt))\n",
    "for i in range(ndata-1):\n",
    "    # print(df.iloc[i][\"trace_name\"])\n",
    "    print(\"Done at %f\"%(i/ndata))\n",
    "    Z[i,:]=read_data(df.iloc[i][\"trace_name\"],f)[2,:nt]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backup with mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits,fetch_openml\n",
    "digits = load_digits()\n",
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore data type\n",
    "data,y = digits[\"data\"].copy(),digits[\"target\"].copy()\n",
    "print(type(data[0][:]),type(y[0]))\n",
    "# note that we do not modify the raw data that is stored on the digits dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(data[0]),max(data[0]))\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit_transform(data)# fit the model for data normalization\n",
    "newdata = scaler.transform(data) # transform the data. watch that data was converted to a numpy array\n",
    "\n",
    "# Split data into 50% train and 50% test subsets\n",
    "print(f\"There are {data.shape[0]} data samples\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, y, test_size=0.2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Support Vector Machine classifier\n",
    "clf = SVC(gamma=0.001) # model design\n",
    "clf.fit(X_train, y_train) # learn\n",
    "svc_prediction = clf.predict(X_test) # predict on test\n",
    "print(\"SVC Accuracy:\", metrics.accuracy_score(y_true=y_test ,y_pred=svc_prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what are the parameters we are trying to optimize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A search consists of:\n",
    "\n",
    "* an estimator (regressor or classifier such as ``SVC()``);\n",
    "\n",
    "* a parameter space;\n",
    "\n",
    "* a method for searching or sampling candidates;\n",
    "\n",
    "* a cross-validation scheme; and\n",
    "\n",
    "* a loss function.\n",
    "\n",
    "There are two main approaches:\n",
    "- Grid Search cross validation. Performs the search in the brute-force way using cross-validation. One has to define the parameter space. The scikit-learn function is ``GridSearchCV``. More details [here](!https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "  {'C': [1, 5,10,50, 100,500, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1,5, 10,50, 100,500, 1000], 'gamma': [0.01,0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(clf, param_grid, cv=5,verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.fit(X_train, y_train) # learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second approach is the Random Search Cross Validation.\n",
    "It performs the search in the brute-force way using cross-validation. One has to define the parameter space. The scikit-learn function is ``GridSearchCV``. More details [here](!https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform,norm ,loguniform\n",
    "\n",
    " \n",
    "distributions= [ {'C': uniform(loc=1, scale=1000), 'kernel': ['linear']},\n",
    "  {'C': uniform(loc=1, scale=1000), 'gamma': loguniform(1e-4,1e-2), 'kernel': ['rbf']}]\n",
    "clf2 = RandomizedSearchCV(clf, distributions, random_state=0,cv=5)\n",
    "clf2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(search.best_params_)\n",
    "print(clf2.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlgeo_sk')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6825af05e3ea1f79f5651bdd3095330bdaee3a1e3958825bcb0ebbb42c21bf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
